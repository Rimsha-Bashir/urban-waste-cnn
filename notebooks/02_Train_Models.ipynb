{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f1143d-610e-43d5-80ed-4a3882398929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11745d1-892e-4e98-bd21-f341e95055f7",
   "metadata": {},
   "source": [
    "## set paths and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9574ebde-2217-4697-b966-3ae5bf5b141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths \n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "\n",
    "data_root = os.path.join(project_root, \"data\", \"raw\", \"AerialWaste\")\n",
    "\n",
    "image_dirs = [os.path.join(data_root, f\"images{i}\") for i in range(6)]\n",
    "\n",
    "train_json = f'{data_root}/training.json'\n",
    "\n",
    "test_json = f'{data_root}/testing.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a592999b-bf87-421d-a2e6-160532694705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants \n",
    "\n",
    "image_size = 244 \n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf741ff2-dc68-45c3-8ffa-eaa2c3e45d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751b11f-5e08-4954-b049-472444e297d1",
   "metadata": {},
   "source": [
    "## get usable image paths from json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b52865-a0a9-457b-a687-52436ae9eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(file_name, image_dirs):\n",
    "    for dir_path in image_dirs:\n",
    "        full_path = os.path.join(dir_path, file_name)\n",
    "        if os.path.exists(full_path):\n",
    "            return full_path\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa753b9f-8b27-4bfd-8532-78d669688fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>full_path</th>\n",
       "      <th>waste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name                                          full_path  waste\n",
       "0     2.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1\n",
       "1     3.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1\n",
       "2     4.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1\n",
       "3     5.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1\n",
       "4     6.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(train_json, \"r\") as f:\n",
    "    train_json_data = json.load(f)\n",
    "\n",
    "records = []\n",
    "\n",
    "for img in train_json_data[\"images\"]:\n",
    "    path = get_image_path(img[\"file_name\"], image_dirs)\n",
    "\n",
    "    if path is not None:\n",
    "        records.append({\n",
    "            \"file_name\": img[\"file_name\"],\n",
    "            \"full_path\": path,\n",
    "            \"waste\": int(img[\"is_candidate_location\"])\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f58983e-887d-4995-bbc4-6987e640b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usable training images: 6327\n",
      "waste\n",
      "0    4205\n",
      "1    2122\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Total usable training images:\", len(df))\n",
    "print(df[\"waste\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c12b64-4d48-423a-a500-60d55a885c80",
   "metadata": {},
   "source": [
    "## train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150d7227-0390-42e1-977a-d742919150e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5061\n",
      "Validation size: 5061\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"waste\"],\n",
    "    random_state=seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5c9fc-7921-4362-adad-94d36d4c086c",
   "metadata": {},
   "source": [
    "## image transformation + resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbeaf886-8c3d-4979-b6c2-9c5addecee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),      # rotate Â±15 degrees\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.1), # slight brightness/contrast/saturation changes\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b125e77-10c1-48be-8ed5-c2a12bc7b50a",
   "metadata": {},
   "source": [
    "## prepare training & val images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d43603-a110-426d-80b6-ef48544cb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    img = Image.open(row['full_path']).convert('RGB') # To make sure every image has exactly 3 channels (Red, Green, Blue), \n",
    "    # which is what CNNs pretrained on ImageNet expect. \n",
    "    img_tensor = train_transforms(img)  # apply your resizing + normalization\n",
    "    train_images.append(img_tensor)\n",
    "    train_labels.append(row['waste'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5cb89-7b6e-4ebe-87e2-5ff6a25430a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = []\n",
    "val_labels = []\n",
    "\n",
    "for idx, row in val_df.iterrows():\n",
    "    img = Image.open(row['full_path']).convert('RGB') # To make sure every image has exactly 3 channels (Red, Green, Blue), \n",
    "    # which is what CNNs pretrained on ImageNet expect. \n",
    "    img_tensor = val_transforms(img)  # apply your resizing + normalization\n",
    "    val_images.append(img_tensor)\n",
    "    val_labels.append(row['waste'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f51501f-833d-4d55-8338-ebd75d8e72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tensor\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "\n",
    "# pytorch labels need to be tensors - (aslo for loss or gradient measurements, they need to be tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b02bdc0-88c9-4b7e-8981-40217545cddb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mstack(train_images), train_labels)\n\u001b[1;32m----> 2\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43mval_images\u001b[49m), val_labels)\n\u001b[0;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_images' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(torch.stack(train_images), train_labels)\n",
    "val_dataset = TensorDataset(torch.stack(val_images), val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30143402-5d3c-4f5b-b0c2-69178461b6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf776886-df47-43de-9a08-eab337b2718a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
