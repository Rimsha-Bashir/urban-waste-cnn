{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f1143d-610e-43d5-80ed-4a3882398929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from datasets.waste_dataset import WasteDataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11745d1-892e-4e98-bd21-f341e95055f7",
   "metadata": {},
   "source": [
    "## set paths and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9574ebde-2217-4697-b966-3ae5bf5b141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths \n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(cwd, \"..\"))\n",
    "\n",
    "data_root = os.path.join(project_root, \"data\", \"raw\", \"AerialWaste\")\n",
    "\n",
    "image_dirs = [os.path.join(data_root, f\"images{i}\") for i in range(6)]\n",
    "\n",
    "train_json = f'{data_root}/training.json'\n",
    "\n",
    "test_json = f'{data_root}/testing.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a592999b-bf87-421d-a2e6-160532694705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants \n",
    "\n",
    "image_size = 244 \n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf741ff2-dc68-45c3-8ffa-eaa2c3e45d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751b11f-5e08-4954-b049-472444e297d1",
   "metadata": {},
   "source": [
    "## get usable image paths from json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b52865-a0a9-457b-a687-52436ae9eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(file_name, image_dirs):\n",
    "    for dir_path in image_dirs:\n",
    "        full_path = os.path.join(dir_path, file_name)\n",
    "        if os.path.exists(full_path):\n",
    "            return full_path\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa753b9f-8b27-4bfd-8532-78d669688fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>full_path</th>\n",
       "      <th>waste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.png</td>\n",
       "      <td>C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name                                          full_path  waste\n",
       "0     2.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1\n",
       "1     3.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1\n",
       "2     4.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1\n",
       "3     5.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1\n",
       "4     6.png  C:\\Users\\rimsh\\Desktop\\rimsha\\github\\urban-was...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(train_json, \"r\") as f:\n",
    "    train_json_data = json.load(f)\n",
    "\n",
    "records = []\n",
    "\n",
    "for img in train_json_data[\"images\"]:\n",
    "    path = get_image_path(img[\"file_name\"], image_dirs)\n",
    "\n",
    "    if path is not None:\n",
    "        records.append({\n",
    "            \"file_name\": img[\"file_name\"],\n",
    "            \"full_path\": path,\n",
    "            \"waste\": int(img[\"is_candidate_location\"])\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f58983e-887d-4995-bbc4-6987e640b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usable training images: 6327\n",
      "waste\n",
      "0    4205\n",
      "1    2122\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Total usable training images:\", len(df))\n",
    "print(df[\"waste\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c12b64-4d48-423a-a500-60d55a885c80",
   "metadata": {},
   "source": [
    "## train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "150d7227-0390-42e1-977a-d742919150e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5061\n",
      "Validation size: 5061\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"waste\"],\n",
    "    random_state=seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5c9fc-7921-4362-adad-94d36d4c086c",
   "metadata": {},
   "source": [
    "## image transformation + resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbeaf886-8c3d-4979-b6c2-9c5addecee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),      # rotate Â±15 degrees\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.1), # slight brightness/contrast/saturation changes\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b125e77-10c1-48be-8ed5-c2a12bc7b50a",
   "metadata": {},
   "source": [
    "## prepare training & val images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae44c9c-240a-48df-84c7-999b02335358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row[\"full_path\"]).convert(\"RGB\")\n",
    "        label = row[\"waste\"]\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "408c6b54-9741-4df5-8aa0-bbeb5f038d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WasteDataset(train_df, train_transforms)\n",
    "val_dataset = WasteDataset(val_df, val_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39b979-4b99-403e-a17c-e836652ee458",
   "metadata": {},
   "source": [
    "## RESENET18 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67ffc1c-86b2-40e5-8f26-e2e7e1cea6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Freeze backbone\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace classifier\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca215a8-96e7-4266-8060-82134887235b",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2b75ded-8516-41f6-a274-73f60aa369fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.fc.parameters(),\n",
    "    lr=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a84732a4-832f-4891-ad1a-ef0dc298b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "543b084b-d993-4f58-bfe6-7310bd942c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            preds.extend(predictions)\n",
    "            targets.extend(labels.numpy())\n",
    "\n",
    "    return accuracy_score(targets, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d80e34-c883-4a72-9cf0-a3d833a5ca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 0/159 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf776886-df47-43de-9a08-eab337b2718a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
